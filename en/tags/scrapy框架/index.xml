<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Scrapy框架 on Luenci</title>
    <link>https://luenci.me/en/tags/scrapy%E6%A1%86%E6%9E%B6/</link>
    <description>Recent content in Scrapy框架 on Luenci</description>
    <generator>Hugo -- 0.133.0</generator>
    <language>en</language>
    <atom:link href="https://luenci.me/en/tags/scrapy%E6%A1%86%E6%9E%B6/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>京东全网爬虫</title>
      <link>https://luenci.me/en/posts/%E4%BA%AC%E4%B8%9C%E5%85%A8%E7%BD%91%E7%88%AC%E8%99%AB/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://luenci.me/en/posts/%E4%BA%AC%E4%B8%9C%E5%85%A8%E7%BD%91%E7%88%AC%E8%99%AB/</guid>
      <description>完整代码见： https://github.com/Lucareful/JingDongSpider 写在前面： 折腾了很久的用python做爬虫项目到现在也该告一段落了，看视频学，遇到bug自己查找，代码思路不对重新写，环境不对</description>
      <content:encoded><![CDATA[<p>完整代码见： <a href="https://github.com/Lucareful/JingDongSpider">https://github.com/Lucareful/JingDongSpider</a></p>
<p>写在前面：</p>
<blockquote>
<p>折腾了很久的用python做爬虫项目到现在也该告一段落了，看视频学，遇到bug自己查找，代码思路不对重新写，环境不对自己配置&hellip;.一路上跌跌撞撞，过程很艰苦，所幸结果为好。</p>
<p>代码就像一面明镜，照见我自身的不足。继续加油</p>
</blockquote>
<h2 id="需求">需求</h2>
<ul>
<li>抓取首页的分类信息
<ul>
<li>大分类的url</li>
<li>中分类的url</li>
<li>小分类的url</li>
</ul>
</li>
<li>抓取商品信息
<ul>
<li>商品名称</li>
<li>价格</li>
<li>评论信息</li>
<li>店铺</li>
<li>促销</li>
<li>选项</li>
<li>图片</li>
</ul>
</li>
</ul>
<h2 id="开发环境和技术">开发环境和技术</h2>
<ul>
<li>技术选择：</li>
<li>由于全网爬虫，抓取页面非常多，为了提高抓的速度，选择使用<code>scrapy</code>框架+<code>scrapy_redis</code>分布式组件</li>
<li>由于京东全网的数据量达到了亿级，存储又是结构化数据，</li>
<li>数据库，选择使用MongoDB；</li>
</ul>
<!-- more -->
<h1 id="京东全网爬虫的实现步骤">京东全网爬虫的实现步骤</h1>
<ul>
<li>采取广度优先策略，我们把类别和商品信息的抓取分开来做</li>
</ul>
<blockquote>
<p>优点，提高程序的稳定性</p>
</blockquote>
<ul>
<li>总体设计</li>
</ul>
<p><img loading="lazy" src="https://i.ibb.co/FDqW9vd/image-20191120143929099.png" alt="image-20191120143929099"  />
</p>
<h2 id="实现步骤">实现步骤</h2>
<ol>
<li>创建爬虫项目</li>
<li>根据需求，定义数据数据模型</li>
<li>实现分类起虫</li>
<li>保存分类信息</li>
<li>实现商品爬虫</li>
<li>保存商品信息</li>
<li>实现随机User-Agent和代理IP下载器中间件，解决IP反爬。</li>
</ol>
<h2 id="创建爬虫项目">创建爬虫项目</h2>
<p><code>scrapy startproject jingDong</code></p>
<h2 id="定义数据模型要抓取的数据">定义数据模型（要抓取的数据）</h2>
<h3 id="类别数据模型类">类别数据模型类</h3>
<ul>
<li>
<p>用于存储类别信息（Category）-字段：</p>
</li>
<li>
<p><code>b.category_name</code>：大类别名称。</p>
</li>
<li>
<p><code>b_category_url</code>：大类别URL。</p>
</li>
<li>
<p><code>m_category_name</code>：中分类名称。</p>
</li>
<li>
<p><code>m_category_url</code>：中分类URL。</p>
</li>
<li>
<p><code>s_category_name</code>：小分类名称</p>
</li>
<li>
<p><code>s_category_url</code>：小分类URL</p>
</li>
</ul>
<h3 id="商品数据模型">商品数据模型</h3>
<ul>
<li>商品数据模型类：用于存储商品信息（Product）字段：</li>
<li>oproduct_category：商品类别</li>
<li>product_sku_id：商品ID</li>
<li>product_name：商品名称</li>
<li>product_img_url：商品图片URL</li>
<li>product_book_info：图书信息，作者，出版社</li>
<li>product_option：商品选项</li>
<li>product_shop：商品店铺</li>
<li>product_comments：商品评论数量</li>
<li>product_ad：商品促销</li>
<li>product_price：商品价格</li>
</ul>
<h2 id="商品的分类爬虫">商品的分类爬虫</h2>
<ul>
<li>创建爬虫</li>
<li>进入项目目录：cd mall_spider</li>
<li>创建爬虫：scrapy genslider category_spider jd.com</li>
<li>指定起始URL</li>
<li>修改起始URL: <a href="https://dc.3.cn/category/get">https://dc.3.cn/category/get</a></li>
</ul>
<h2 id="实现保存分类的pipeline类">实现保存分类的pipeline类</h2>
<ul>
<li>open_spider 方法中，链接MongoDB数据库，获取要操作的集合</li>
<li>process_item 方法中，向MongoDB中插入类别数据</li>
<li>close_spider 方法中，关闭MongoDB的链接</li>
</ul>
<h1 id="实现商品爬虫">实现商品爬虫</h1>
<ul>
<li>
<p>步骤</p>
<ul>
<li>分析，确定数据所在的URL</li>
<li>代码实现（核心）</li>
<li>商品爬虫实现分布式</li>
</ul>
</li>
<li>
<p>分析，确定数据所在的URL</p>
<ul>
<li>解析列表页，提取商品sku_id，实现翻页，确定翻页的URL</li>
<li>获取商品的基本信息，通过手机抓包（APP），确定URL</li>
<li>PC详情页面，确定商品的促销信息的URL</li>
<li>PC详情页面，确定评论信息的URL</li>
<li>PC详情页面，确定商品价格信息的URL</li>
</ul>
</li>
<li>
<p>代码实现</p>
</li>
<li>
<p>1.重写start_requests方法，根据分类信息构建列表页的请求</p>
</li>
<li>
<p>2.解析列表页，提取商品的skuid，构建商品基本的信息请求；实现列表翻页</p>
<p>1.确定商品基本的信息请求</p>
<p>1.URL:https://cdnware.m.jd.com/c1/skuDetail/apple/7.3.0/32962088964.json
2.请求方法：GET
3.参数/数据：32962088964商品的skuid
2.解析列表页，提取商品的skuid
3.构建商品基本的信息请求
4.实现列表翻页</p>
</li>
<li>
<p>解析促销信息，构建商品评价信息的请求</p>
</li>
<li>
<p>1.解析促销信息</p>
<ul>
<li>
<p>1.produft_ad：商品促销</p>
</li>
<li>
<p>2.构建商品评价信息的请求</p>
<ul>
<li>1.准备评价信息的请求</li>
</ul>
</li>
</ul>
</li>
<li>
<p>解析商品评价信息，构建价格信息的请求</p>
</li>
<li>
<p>解析商品评价信息</p>
<ul>
<li>1.product_comments：商品评论数量</li>
<li>2.评价数量，好评数量，差拜数量，好评率</li>
<li>2.构建价格信息的请求</li>
</ul>
</li>
<li>
<p>准备价格请求：</p>
<ul>
<li>1.URL:https://p.3.cn/prices/mgets?skulds=J_69334292.</li>
<li>2.请求方法：GET</li>
<li>3.参数：skulds=J_6933429，j后跟这个商品的sku_id</li>
</ul>
</li>
<li>
<p>解析价格信息</p>
<ul>
<li>1.product_price：商品价格</li>
<li>2.把商品数据交给引擎</li>
</ul>
</li>
</ul>
<h1 id="商品爬虫实现分布式">商品爬虫实现分布式</h1>
<ul>
<li>修改爬虫类
<ul>
<li>修改继承关系</li>
<li>指定redis_key</li>
<li>把重写start_requests改为重写make_request from data I</li>
</ul>
</li>
<li>在settings文件中配置scrapy_redis
<ul>
<li>直接拷贝scrapy_redis配置信息，到settings.py中.</li>
</ul>
</li>
<li>写一个程序用于把MongoDB中分类信息，放入到爬虫redis_key指定的列表中</li>
</ul>
<h1 id="保存商品数据">保存商品数据</h1>
<ul>
<li>实现存储商品Pipeline类
<ul>
<li>在open_spider方法，建立MongoDB数据库连接，获取要操作的集合</li>
<li>在process_item方法，把数据插入到MongoDB中</li>
<li>在close_spider方法，关闭数据库连接</li>
</ul>
</li>
<li>在settings.py中开启这个管道</li>
</ul>
<h1 id="实现下载器中间件">实现下载器中间件</h1>
<ul>
<li>
<p>实现随机User-Agent的中间件</p>
</li>
<li>
<p>在settings.py中开启上面的下载器中间
件</p>
</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
