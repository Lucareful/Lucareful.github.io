<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Hadoop on Luenci</title>
    <link>https://luenci.com/en/categories/hadoop/</link>
    <description>Recent content in Hadoop on Luenci</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en</language>
    <atom:link href="https://luenci.com/en/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大数据之Hadoop</title>
      <link>https://luenci.com/en/posts/hadoop%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://luenci.com/en/posts/hadoop%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/</guid>
      <description>&lt;h1 id=&#34;hadoop介绍&#34;&gt;Hadoop介绍&lt;/h1&gt;
&lt;h2 id=&#34;组成部分&#34;&gt;组成部分&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop主要由3部分组成：
&lt;ul&gt;
&lt;li&gt;Mapreduce编程模型&lt;/li&gt;
&lt;li&gt;HDFS分布式文件存储&lt;/li&gt;
&lt;li&gt;YARN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<h1 id="hadoop介绍">Hadoop介绍</h1>
<h2 id="组成部分">组成部分</h2>
<ul>
<li>Hadoop主要由3部分组成：
<ul>
<li>Mapreduce编程模型</li>
<li>HDFS分布式文件存储</li>
<li>YARN</li>
</ul>
</li>
</ul>
<h2 id="配置信息">配置信息</h2>
<p>管理员密码：admin123</p>
<p>DNS1=202.103.24.68
DNS2=202.103.44.150</p>
<p>export HADOOP_HOME=/usr/local/hadoop-2.6.4
export PATH=$HADOOP_HOME/bin$PATH:/usr/java/jdk1.8.0_201-amd64/bin</p>
<h2 id="hadoop-hdfs-简介">Hadoop HDFS 简介</h2>
<p><strong>概念：</strong></p>
<p><strong>namenode</strong>：负责记录 数据块 的分布情况– 数据元数据信息
<strong>datanode</strong>:负责实际存储 数据块
<strong>block</strong>：是Hadoop最小存储数据单位 默认 128M
<strong>secondarynamenode</strong>: 辅助namenode完成fsimage管理或者优化</p>
<h2 id="hdfs简单命令">HDFS简单命令</h2>
<ul>
<li>
<p>hadoop  version   //查看版本</p>
</li>
<li>
<p>hadoop fs        //文件系统客户端</p>
</li>
<li>
<p>hadoop  jar    //运行jar包</p>
</li>
<li>
<p>hadoop classpath //查看类路径</p>
</li>
<li>
<p>hadoop checknative //检查本地库并压缩</p>
</li>
<li>
<p>hadoop distcp   // 远程递归拷贝文件</p>
</li>
<li>
<p>hadoop credential //认证</p>
</li>
<li>
<p>hadoop trace    //跟踪</p>
</li>
<li>
<p>$ hdfs dfs -mkdir-p /user/ubuntu/        //在hdfs上建立文件夹</p>
</li>
<li>
<p>$ hdfs dfs -puthdfs.cmd  /user/ubuntu/  //将本地文件上传到HDFS</p>
</li>
<li>
<p>$ hdfs dfs -get/user/ubuntu/hadoop.cmd  a.cmd   //将文件从HDFS取回本地</p>
</li>
<li>
<p>$  hdfs dfs -rm -r  -f  /user/ubuntu/    //删除</p>
</li>
<li>
<p>$ hdfs dfs -ls -R/                       //递归展示HDFS文件系统</p>
</li>
</ul>
<p>一、hadoop所在目录
cd usr/local/hadoop
1</p>
<p>二、启动hadoop
bash ./starth.sh<br>
% 运行start-dfs.sh
% 运行start-yarn.sh
1
2
3</p>
<p>启动dfs，浏览器查看：</p>
<p>172.16.31.17:50070
1</p>
<p>启动脚本，浏览器查看：</p>
<p>172.16.31.17:8088
1</p>
<p>停止脚本：</p>
<p>bash ./sweighth.sh
1
三、常用命令</p>
<p>1.显示hadoop目录结构</p>
<p>hdfs dfs -ls -R /
1</p>
<p>2.在hadoop指定目录内创建新目录</p>
<p>hdfs dfs -mkdir /winnie
1</p>
<p>3.将本地文件夹存储至hadoop</p>
<p>hdfs dfs -put [本地目录] [hadoop目录]
1</p>
<p>4.将本地文件存储至hadoop</p>
<p>hdfs dfs -put [本地地址] [hadoop目录]
1</p>
<p>5.查看指定目录下内容</p>
<p>hdfs dfs -ls [文件目录]
1</p>
<p>6.打开某个已存在文件</p>
<p>hdfs dfs -cat [file_path]
1</p>
<p>7.在hadoop指定目录下新建一个空文件</p>
<p>hdfs dfs -touchz /winnie/test03.txt
1</p>
<p>8.将hadoop上某个文件重命名</p>
<p>hdfs dfs -mv /winnie/test03.txt /winnie/test.txt
1</p>
<p>9.将hadoop上某个文件down至本地已有目录下</p>
<p>hdfs dfs -get [文件目录] [本地目录]
1</p>
<p>10.将hadoop指定目录下所有内容保存为一个文件，同时down至本地</p>
<p>hdfs dfs -getmerge /winnie/hadoop-file /home/spark/hadoop-file/test.txt
1</p>
<p>11.删除hadoop上指定文件</p>
<p>hdfs dfs -rm [文件地址]
1</p>
<p>12.删除hadoop上指定文件夹（包含子目录等）</p>
<p>hdfs dfs -rm -r [目录地址]
hdfs dfs -rmr [目录地址]
1
2</p>
<p>13.将正在运行的hadoop作业kill掉</p>
<p>hadoop job -kill [job-id]
1
14.查看帮助</p>
<p>hdfs dfs -help
1
四、安全模式
1.退出安全模式</p>
<p>NameNode在启动时会自动进入安全模式，安全模式是NameNode的一种状态，在这个阶段，文件系统不允许有任何修改。</p>
<p>系统显示Name node in safe mode，说明系统正处于安全模式，这时只需要等待几十秒即可，也可通过下面的命令退出安全模式：</p>
<p>/usr/local/hadoop$bin/hadoop dfsadmin -safemode leave
1</p>
<p>2.进入安全模式</p>
<p>在必要情况下，可以通过以下命令吧HDFS置于安全模式：</p>
<p>/usr/local/hadoop$bin/hadoop dfsadmin -safemode enter
1</p>
<p>五、补充</p>
<p>1.对hdfs操作的命令格式是hdfs dfs</p>
<p>1.1 -ls 表示对hdfs下一级目录的查看
1.2 -lsr 表示对hdfs目录的递归查看
1.3 -mkdir 创建目录
1.4 -put 从Linux上传文件到hdfs
1.5 -get 从hdfs下载文件到linux
1.6 -text 查看文件内容
1.7 -rm 表示删除文件
1.7 -rmr 表示递归删除文件</p>
<p>2.hdfs在对数据存储进行block划分时，如果文件大小超过block，那么按照block大小进行划分；不如block size的，划分为一个块，是实际数据大小。</p>
<p>3.hadoop常用命令：</p>
<p>hdfs dfs  查看Hadoop HDFS支持的所有命令<br>
hdfs dfs –ls  列出目录及文件信息<br>
hdfs dfs –lsr  循环列出目录、子目录及文件信息   <br>
hdfs dfs –tail /user/sunlightcs/test.txt  查看最后1KB的内容</p>
<p>hdfs dfs –copyFromLocal test.txt /user/sunlightcs/test.txt  从本地文件系统复制文件到HDFS文件系统，等同于put命令<br>
hdfs dfs –copyToLocal /user/sunlightcs/test.txt test.txt  从HDFS文件系统复制文件到本地文件系统，等同于get命令</p>
<p>hdfs dfs –chgrp [-R] /user/sunlightcs  修改HDFS系统中/user/sunlightcs目录所属群组，选项-R递归执行，跟linux命令一样<br>
hdfs dfs –chown [-R] /user/sunlightcs  修改HDFS系统中/user/sunlightcs目录拥有者，选项-R递归执行<br>
hdfs dfs –chmod [-R] MODE /user/sunlightcs  修改HDFS系统中/user/sunlightcs目录权限，MODE可以为相应权限的3位数或+/-{rwx}，选项-R递归执行</p>
<p>hdfs dfs –count [-q] PATH  查看PATH目录下，子目录数、文件数、文件大小、文件名/目录名<br>
hdfs dfs –cp SRC [SRC …] DST       将文件从SRC复制到DST，如果指定了多个SRC，则DST必须为一个目录<br>
hdfs dfs –du PATH  显示该目录中每个文件或目录的大小<br>
hdfs dfs –dus PATH  类似于du，PATH为目录时，会显示该目录的总大小</p>
<p>hdfs dfs –expunge  清空回收站，文件被删除时，它首先会移到临时目录.Trash/中，当超过延迟时间之后，文件才会被永久删除</p>
<p>hdfs dfs –getmerge SRC [SRC …] LOCALDST [addnl]   获取由SRC指定的所有文件，将它们合并为单个文件，并写入本地文件系统中的LOCALDST，选项addnl将在每个文件的末尾处加上一个换行符</p>
<p>hdfs dfs –test –[ezd] PATH     对PATH进行如下类型的检查：-e PATH是否存在，如果PATH存在，返回0，否则返回1；-z 文件是否为空，如果长度为0，返回0，否则返回1； -d 是否为目录，如果PATH为目录，返回0，否则返回1</p>
<p>hdfs dfs –text PATH  显示文件的内容，当文件为文本文件时，等同于cat；文件为压缩格式（gzip以及hadoop的二进制序列文件格式）时，会先解压缩</p>
<p>hdfs dfs –help ls  查看某个[ls]命令的帮助文档</p>
<h2 id="mapreduce解释">Mapreduce解释</h2>
<ul>
<li>
<p>**mapper的角色:**hadoop将用户提交的mapper可执行程序或脚本作为一个单独的进程加载起来，这个进程我们称之为mapper进程，hadoop不断地将文件片段转换为行，传递到我们的mapper进程中，mapper进程通过标准输入的方式一行一行地获取这些数据，然后设法将其转换为键值对，再通过标准输出的形式将这些键值对按照一对儿一行的方式输出出去。</p>
</li>
<li>
<p>虽然在我们的mapper函数中，我们自己能分得清key/value(比方说有可能在我们的代码中使用的是string key,int value)，但是当我们采用标准输出之后，key value是打印到一行作为结果输出的(比如sys.stdout.write(&quot;%s\t%s\n&quot;%(birthyear,gender)))，因此我们为了保证hadoop能从中鉴别出我们的键值对，键值对中一定要以分隔符&rsquo;\t&rsquo;即Tab(也可自定义分隔符)字符分隔，这样才能保证hadoop正确地为我们进行partitoner、shuffle等等过程。</p>
</li>
<li>
<p>**reducer的角色:**hadoop将用户提交的reducer可执行程序或脚本同样作为一个单独的进程加载起来，这个进程我们称之为reducer进程，hadoop不断地将键值对(按键排序)按照一对儿一行的方式传递到reducer进程中，reducer进程同样通过标准输入的方式按行获取这些键值对儿，进行自定义计算后将结果通过标准输出的形式输出出去。</p>
</li>
<li>
<p>在reducer这个过程中需要注意的是：传递进reducer的键值对是按照键排过序的，这点是由MR框架的sort过程保证的，因此如果读到一个键与前一个键不同，我们就可以知道当前key对应的pairs已经结束了，接下来将是新的key对应的pairs。</p>
</li>
</ul>]]></content:encoded>
    </item>
  </channel>
</rss>
